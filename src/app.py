import os
# --- 0. æ ¸å¿ƒä¿®å¤ï¼šè·³è¿‡ PyTorch å®‰å…¨æ£€æŸ¥ï¼ˆè§£å†³ CVE-2025-32434 æŠ¥é”™ï¼‰ ---
os.environ["TRANSFORMERS_SKIP_TORCH_LOAD_CHECK"] = "True"

import streamlit as st
import pandas as pd
import torch
import faiss
import sys
import pickle
import base64
from PIL import Image
from openai import OpenAI
import dashscope
from dashscope import TextEmbedding
from transformers import CLIPProcessor, CLIPModel
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain_core.embeddings import Embeddings

# --- 1. åŸºç¡€é…ç½® ---
st.set_page_config(page_title="ä¸“ä¸šçš®è‚¤é•œå½±åƒåˆ†æä¸“å®¶", page_icon="ğŸ”¬", layout="wide")

ALIYUN_API_KEY = os.env.get("ALIYUN_API_KEY")
dashscope.api_key = ALIYUN_API_KEY
client = OpenAI(
    api_key=ALIYUN_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# --- 2. æ–‡æœ¬å‘é‡å·¥å…· (ç”¨äº PDF æ£€ç´¢) ---
class AliyunEmbedding(Embeddings):
    def embed_documents(self, texts):
        all_embeddings = []
        for i in range(0, len(texts), 10):
            batch = texts[i : i + 10]
            resp = TextEmbedding.call(model='text-embedding-v3', input=batch)
            if resp.status_code == 200:
                all_embeddings.extend([item['embedding'] for item in resp.output['embeddings']])
            else:
                raise Exception(f"Embedding Error: {resp.message}")
        return all_embeddings

    def embed_query(self, text):
        if not text.strip(): return [0] * 1536
        resp = TextEmbedding.call(model='text-embedding-v3', input=[text])
        return resp.output['embeddings'][0]['embedding'] if resp.status_code == 200 else None

embeddings_tool = AliyunEmbedding()

# --- 3. ç¼“å­˜åˆå§‹åŒ–é€»è¾‘ ---

@st.cache_resource
def init_knowledge_bases():
    """åŒæ—¶åˆå§‹åŒ–æ–‡æœ¬åº“å’Œè§†è§‰åº“"""
    # A. åˆå§‹åŒ–æ–‡æœ¬åº“ (PDF)
    text_db = None
    index_path = "dermo_faiss_index"
    pdf_files = ["data/dermoscopy_atlas_1.pdf", "data/dermoscopy_atlas_2.pdf"] 
    
    if os.path.exists(index_path):
        text_db = LangChainFAISS.load_local(index_path, embeddings_tool, allow_dangerous_deserialization=True)
    else:
        found_pdfs = [f for f in pdf_files if os.path.exists(f)]
        if found_pdfs:
            all_docs = []
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)
            for pdf in found_pdfs:
                loader = PyPDFLoader(pdf)
                all_docs.extend(text_splitter.split_documents(loader.load()))
            text_db = LangChainFAISS.from_texts([d.page_content for d in all_docs], embeddings_tool)
            text_db.save_local(index_path)

    # B. åˆå§‹åŒ–è§†è§‰åº“ (HAM10000)
    v_index, v_paths, v_meta, v_model, v_processor, v_device = None, None, None, None, None, None
    v_idx_file = "image_index/visual_kb.index"
    v_pkl_file = "image_index/image_paths.pkl"
    v_csv_file = "HAM10000_metadata.csv"

    if os.path.exists(v_idx_file) and os.path.exists(v_pkl_file):
        v_index = faiss.read_index(v_idx_file)
        with open(v_pkl_file, "rb") as f:
            v_paths = pickle.load(f)
        v_meta = pd.read_csv(v_csv_file) if os.path.exists(v_csv_file) else None
        
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        # å¼ºåˆ¶ä½¿ç”¨ safetensors=True å¢åŠ å®‰å…¨æ€§
        v_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32", use_safetensors=True).to(device)
        v_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        v_device = device

    return text_db, v_index, v_paths, v_meta, v_model, v_processor, v_device

# åŠ è½½æ‰€æœ‰åº“
text_db, v_index, v_paths, v_meta, v_model, v_processor, v_device = init_knowledge_bases()

# --- 4. ç•Œé¢å±•ç¤º ---
st.title("ğŸ”¬ çš®è‚¤é•œå½±åƒæ™ºèƒ½ä¸“å®¶ç³»ç»Ÿ")
st.caption("å·²å¯ç”¨ï¼šPDFæ•™æçŸ¥è¯†æ£€ç´¢ + HAM10000 ç›¸ä¼¼ç—…ä¾‹æ¯”å¯¹")

with st.sidebar:
    st.header("ğŸ“¸ å½±åƒä¸Šä¼ ")
    uploaded_file = st.file_uploader("ä¸Šä¼ çš®è‚¤é•œç…§ç‰‡", type=["jpg", "png", "jpeg"])
    location = st.selectbox("å‘ç—…éƒ¨ä½", ["å››è‚¢", "èº¯å¹²", "å¤´é¢éƒ¨", "æŒè·–", "ç”²ä¸‹", "ç²˜è†œ"])
    evolution = st.selectbox("è¿‘æœŸå˜åŒ–", ["æ— æ˜æ˜¾å˜åŒ–", "é¢œè‰²åŠ æ·±/ä½“ç§¯å¢å¤§", "è¾¹ç¼˜ä¸å¯¹ç§°", "å‡ºè¡€/ç ´æºƒ"])
    
    if text_db: st.success("ğŸ“– æ•™æåº“å·²å°±ç»ª")
    if v_index: st.success("ğŸ–¼ï¸ è§†è§‰åº“å·²å°±ç»ª")

if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# --- 5. å¯¹è¯æ£€ç´¢é€»è¾‘ ---
if prompt := st.chat_input("æè¿°è¯¦ç»†ç—‡çŠ¶..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # --- A. æ£€ç´¢æ–‡æœ¬å†…å®¹ (PDF) ---
        context_text = ""
        if text_db:
            with st.spinner("æ­£åœ¨æŸ¥é˜…ä¸“å®¶æ•™æ..."):
                search_results = text_db.similarity_search(f"{location} {prompt}", k=2)
                context_text = "\n".join([res.page_content for res in search_results])
        
        # --- B. æ£€ç´¢ç›¸ä¼¼ç—…ä¾‹ (è§†è§‰åº“) ---
        reference_info = ""
        if uploaded_file and v_index is not None:
            with st.spinner("æ­£åœ¨æœç´¢ç›¸ä¼¼ä¸´åºŠç—…ä¾‹..."):
                img = Image.open(uploaded_file).convert("RGB")
                inputs = v_processor(images=img, return_tensors="pt").to(v_device)
                with torch.no_grad():
                    feat = v_model.get_image_features(**inputs)
                    feat /= feat.norm(p=2, dim=-1, keepdim=True)
                    query_emb = feat.cpu().numpy().astype('float32')
                
                D, I = v_index.search(query_emb, 3)
                
                st.write("ğŸ” **åº“å†…ç›¸ä¼¼æ¡ˆä¾‹å‚è€ƒï¼š**")
                cols = st.columns(3)
                dx_map = {"mel": "é»‘è‰²ç´ ç˜¤", "nv": "é»‘è‰²ç´ ç—£", "bcc": "åŸºåº•ç»†èƒç™Œ", "akiec": "æ—¥å…‰æ€§è§’åŒ–ç—…", "bkl": "è‰¯æ€§è§’åŒ–ç—…", "df": "çš®è‚¤çº¤ç»´ç˜¤", "vasc": "è¡€ç®¡ç˜¤"}
                
                for idx, col in enumerate(cols):
                    match_idx = I[0][idx]
                    ref_path = v_paths[match_idx]
                    img_id = os.path.basename(ref_path).replace(".jpg", "")
                    dx_code = v_meta[v_meta['image_id'] == img_id]['dx'].values[0] if v_meta is not None else "æœªçŸ¥"
                    dx_name = dx_map.get(dx_code, dx_code)
                    
                    with col:
                        if os.path.exists(ref_path):
                            st.image(ref_path, caption=f"åŒ¹é…åº¦: {1/(1+D[0][idx]):.2f}")
                            st.info(f"ç¡®è¯Š: {dx_name}")
                        reference_info += f"ç›¸ä¼¼æ¡ˆä¾‹{idx+1}ç¡®è¯Šä¸º{dx_name}; "

        # --- C. ç»¼åˆåˆ†æ (Qwen-VL) ---
        final_prompt = f"""
ä½ æ˜¯ä¸€ä½çš®è‚¤é•œä¸“å®¶ã€‚è¯·ç»¼åˆä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ·±åº¦åˆ†æï¼š

ã€å‚è€ƒæ•™æçŸ¥è¯†ã€‘ï¼š
{context_text if context_text else "æœªæ£€ç´¢åˆ°ç›´æ¥ç›¸å…³çš„æ•™ææ®µè½ã€‚"}

ã€æ•°æ®åº“ç›¸ä¼¼ç—…ä¾‹å‚è€ƒã€‘ï¼š
{reference_info if reference_info else "æœªè¿›è¡Œç›¸ä¼¼ç—…ä¾‹å¯¹æ¯”ã€‚"}

ã€ä¸´åºŠä¿¡æ¯ã€‘ï¼š
éƒ¨ä½ï¼š{location}ï¼Œå˜åŒ–ï¼š{evolution}ï¼Œæ‚£è€…ä¸»è¯‰ï¼š{prompt}

è¯·ç»“åˆå›¾ç‰‡ç»†èŠ‚ï¼Œæè¿°å…¶å…¸å‹çš„çš®è‚¤é•œå¾è±¡ï¼Œç»™å‡ºåˆæ­¥å°è±¡ï¼Œå¹¶æä¾›éšè¯Šå»ºè®®ï¼ˆéœ€åŒ…å«å…è´£å£°æ˜ï¼‰ã€‚
"""
        
        msg_content = [{"type": "text", "text": final_prompt}]
        if uploaded_file:
            b64 = base64.b64encode(uploaded_file.getvalue()).decode()
            msg_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})

        with st.spinner("ä¸“å®¶æ­£åœ¨åˆ†æå½±åƒ..."):
            response = client.chat.completions.create(
                model="qwen-vl-max",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸‰ç”²åŒ»é™¢çš®è‚¤é•œè¯Šæ–­ä¸“å®¶ï¼Œå›å¤ä¸“ä¸šã€å®¢è§‚ã€‚"},
                    {"role": "user", "content": msg_content}
                ]
            )
            
            answer = response.choices[0].message.content
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})